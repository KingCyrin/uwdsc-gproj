{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten a list of two tuples\n",
    "def lst_flatten(lst):\n",
    "    ret = []\n",
    "    for a,b in lst:\n",
    "        ret.append(a)\n",
    "        ret.append(b)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> importing  dataset #1\n",
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
      "by Sir Arthur Conan Doyle\n",
      "(#15 in o\n",
      "\n",
      ">>> importing  dataset #2\n",
      "The frequency distribution for attribute 'lemma' in corpus 'internet-en'\n",
      "For more information visit \n",
      "\n",
      ">>> importing  dataset #3\n",
      "you 6281002\n",
      "i 5685306\n",
      "the 4768490\n",
      "to 3453407\n",
      "a 3048287\n",
      "it 2879962\n",
      "and 2127187\n",
      "that 2030642\n",
      "of 184788\n",
      "\n",
      ">>> importing  dataset #4\n",
      "2\n",
      "1080\n",
      "&c\n",
      "10-point\n",
      "10th\n",
      "11-point\n",
      "12-point\n",
      "16-point\n",
      "18-point\n",
      "1st\n",
      "2,4,5-t\n",
      "2,4-d\n",
      "20-point\n",
      "2D\n",
      "2nd\n",
      "30-30\n",
      "\n",
      "\n",
      ">>> importing  dataset #5\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aah\n",
      "aahed\n",
      "aahing\n",
      "aahs\n",
      "aal\n",
      "aalii\n",
      "aaliis\n",
      "aals\n",
      "aam\n",
      "aani\n",
      "aardvark\n",
      "aardvarks\n",
      "aardwolf\n",
      "aardwolves\n",
      "\n",
      ">>> importing  dataset #6\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aachen\n",
      "aardvark\n",
      "aardvarks\n",
      "aardwolf\n",
      "aardwolves\n",
      "aarhus\n",
      "aaron\n",
      "aaronic\n",
      "aaronical\n",
      "aasvogel\n",
      "aasvo\n",
      "\n",
      ">>> importing  dataset #7\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aachen\n",
      "aardvark\n",
      "aardvarks\n",
      "aaron\n",
      "aback\n",
      "abacus\n",
      "abacuses\n",
      "abaft\n",
      "abalone\n",
      "abandon\n",
      "abandoned\n",
      "aband\n",
      "\n",
      ">>> importing  dataset #8\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aachen\n",
      "aardvark\n",
      "aardvarks\n",
      "aaron\n",
      "aback\n",
      "abacus\n",
      "abacuses\n",
      "abaft\n",
      "abalone\n",
      "abandon\n",
      "abandoned\n",
      "aband\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yuge_data = \"\"\n",
    "\n",
    "cnt = 1\n",
    "dataset_lst = []\n",
    "dataset_lst.append(\"yuge_data.txt\")\n",
    "dataset_lst.append(\"internet-en.num.txt\")\n",
    "dataset_lst.append(\"en.txt\")\n",
    "dataset_lst.append(\"words.txt\")\n",
    "dataset_lst.append(\"words_alpha.txt\")\n",
    "dataset_lst.append(\"english3.txt\")\n",
    "dataset_lst.append(\"english2.txt\")\n",
    "dataset_lst.append(\"usa.txt\")\n",
    "\n",
    "# these dont work (because they contain non utf-8 chars)\n",
    "# dataset_lst.append(\"ukenglish.txt\")\n",
    "# dataset_lst.append(\"engmix.txt\")\n",
    "# dataset_lst.append(\"usa2.txt\")\n",
    "\n",
    "for d in dataset_lst:\n",
    "    data_import = open(d).read()\n",
    "    yuge_data += data_import\n",
    "    print(\">>> importing  dataset #\" + str(cnt))\n",
    "    print(data_import[:100])\n",
    "    print()\n",
    "\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Adventures',\n",
       " 'of',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'by',\n",
       " 'Sir',\n",
       " 'Arthur',\n",
       " 'Conan',\n",
       " 'Doyle',\n",
       " 'in',\n",
       " 'our',\n",
       " 'series',\n",
       " 'by',\n",
       " 'Sir',\n",
       " 'Arthur']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_words(src):\n",
    "    res = src\n",
    "\n",
    "    # allow upper case words\n",
    "    res = re.findall(r'\\b\\w+\\b', src)\n",
    "\n",
    "    # filter out numbers\n",
    "    res = list(itertools.filterfalse(lambda x: x.isnumeric(), res))\n",
    "\n",
    "    # filter out words of length 1\n",
    "    res = list(itertools.filterfalse(lambda x: len(x) == 1, res))\n",
    "\n",
    "    return res\n",
    "\n",
    "extract_words(yuge_data)[:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 72390),\n",
       " ('of', 39844),\n",
       " ('and', 37067),\n",
       " ('to', 28370),\n",
       " ('in', 20227),\n",
       " ('that', 11994),\n",
       " ('was', 11372),\n",
       " ('he', 10062),\n",
       " ('is', 9648),\n",
       " ('his', 9628),\n",
       " ('with', 9501),\n",
       " ('it', 8572),\n",
       " ('as', 7487),\n",
       " ('had', 7337),\n",
       " ('The', 7245),\n",
       " ('for', 6671),\n",
       " ('by', 6651),\n",
       " ('not', 6485),\n",
       " ('at', 6360)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a regex test\n",
    "# tst_str = \"The Project Gutenberg EBook of The Adventures of Sherlock Holmes\\nby Sir Arthur Conan Doyle\\n(#15 in our series by Sir Arthur Conan Doyle)\\n\\nCopyright laws are changing all over the world. Be sure to check the\\ncopyright laws for your country before downloadin\"\n",
    "# re.findall(r'\\b\\w+\\b',tst_str)\n",
    "\n",
    "extract_words(yuge_data)\n",
    "WORD_IDX = Counter(extract_words(yuge_data))\n",
    "\n",
    "# retrieve the most common words\n",
    "WORD_IDX.most_common(19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how likely is a given word?\n",
    "\n",
    "# get the sum of all words\n",
    "ALL_WORD_CNT = sum(WORD_IDX.values())\n",
    "\n",
    "# get the number of occurrences \n",
    "WORD_IDX.values()\n",
    "\n",
    "# probability function\n",
    "def ProbabilityForWord(word):\n",
    "    return WORD_IDX[word]/ALL_WORD_CNT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# building our candidate model\n",
    "# (this boils down to generating possible words \n",
    "#  based on simple string manipulation)\n",
    "def candidates_1(word):\n",
    "    # \"\".join([chr(c) for c in range(65, 91)])\n",
    "    alphab = 'abcdefghijklmnopqrstuvwxyz'+'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "    # edits formed by inserting a space\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    # edits formed by deleting a character\n",
    "    d = [L + R[1:] for L, R in s if R]\n",
    "\n",
    "    # edits formed by swapping the i'th and (i+1)'th characters\n",
    "    # where i < len(word) - 1\n",
    "    t = [L + R[1] + R[0] + R[2:] for L, R in s if len(R)>1] \n",
    "    # ?? consider adding transposes of a longer length \n",
    "    # (i.e., three or four letter transposes)\n",
    "\n",
    "    # edits formed by substituting a random character\n",
    "    # (similar to: \"edits formed by inserting a space\" as shown above)\n",
    "    r = []\n",
    "    for L, R in s:\n",
    "        if R:\n",
    "            for c in alphab:\n",
    "                r.append( L + c + R[1:] )\n",
    "\n",
    "    # edits formed by inserting a random char\n",
    "    i = [L + c + R for L, R in s for c in alphab]\n",
    "\n",
    "    # dont exclude split words\n",
    "    # consider them as candidates\n",
    "    s = lst_flatten(s)\n",
    "\n",
    "    # return all: splits, deletes, edits, replacements, and insertions\n",
    "    return set(s + d + t + r + i)\n",
    "\n",
    "# This code kinda works, but note that when n >= 3, the function \"blows up\".\n",
    "# Finding all edits of distance >= 3 is computationally hard.\n",
    "def candidates_n(word, n):\n",
    "    if (n == 1):\n",
    "        return candidates_1(word)\n",
    "    else: \n",
    "        res = set()\n",
    "        for a in candidates_n(word, n-1):\n",
    "            \n",
    "            # Note: the for loop below is not part of the\n",
    "            # recursion. it's used for flattening the set.\n",
    "            for z in candidates_1(a):\n",
    "                res.add(z)\n",
    "        return res\n",
    "\n",
    "# can a given word be found in the \"dictionary\"?\n",
    "def only_words_in_dict(words):\n",
    "    assert( type(words) == set )\n",
    "    return set(w for w in words if WORD_IDX[w] > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def candidates(word):\n",
    "    res = set()\n",
    "\n",
    "    for e1 in only_words_in_dict(candidates_n(word, 1)):\n",
    "        res.add(e1)\n",
    "    for e2 in only_words_in_dict(candidates_n(word, 2)):\n",
    "        res.add(e2)\n",
    "    # res.add(word)\n",
    "    \n",
    "    # sort according to the following:\n",
    "        # favour longer words over shorter ones\n",
    "        # favour more frequent words over less frequent words\n",
    "    res = sorted(res, key=lambda elmt:(len(elmt), WORD_IDX[elmt]), reverse=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "def best_guess(word):\n",
    "    candidates(word)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProbabilityForWord(\"the\"): 0.026366561125148104\n",
      "ProbabilityForWord(\"pineapple\"): 3.2780639608555458e-06\n",
      "pineapple occurs 9 times in all datasets\n",
      "all words: 2745523\n",
      "[('pin', 52),\n",
      " ('pine', 40),\n",
      " ('el', 10),\n",
      " ('pi', 9),\n",
      " ('pineapple', 9),\n",
      " ('appel', 3),\n",
      " ('pel', 1)]\n",
      "\n",
      "[('pineapples', 8),\n",
      " ('pineappies', 1),\n",
      " ('pineappple', 1),\n",
      " ('pineapple', 9),\n",
      " ('painapple', 1),\n",
      " ('pineaple', 1),\n",
      " ('pinesap', 2)]\n",
      "\n",
      "[('pineapple', 9),\n",
      " ('neapple', 1),\n",
      " ('appeal', 60),\n",
      " ('rappel', 5),\n",
      " ('appels', 3),\n",
      " ('appele', 1),\n",
      " ('Pineda', 1)]\n",
      "\n",
      "[('pineapple', 9),\n",
      " ('neapple', 1),\n",
      " ('rappee', 3),\n",
      " ('appete', 2),\n",
      " ('appere', 2),\n",
      " ('appele', 1),\n",
      " ('Pineda', 1)]\n",
      "\n",
      "[('throat', 81),\n",
      " ('threat', 18),\n",
      " ('thwart', 10),\n",
      " ('thatch', 9),\n",
      " ('thanet', 3),\n",
      " ('thwait', 2),\n",
      " ('thatll', 2)]\n",
      "\n",
      "[('peeled', 9),\n",
      " ('peeler', 6),\n",
      " ('petrel', 5),\n",
      " ('peerly', 3),\n",
      " ('peepul', 3),\n",
      " ('unpeel', 3),\n",
      " ('speels', 3)]\n",
      "\n",
      "[('There', 708),\n",
      " ('These', 236),\n",
      " ('Those', 73),\n",
      " ('Their', 70),\n",
      " ('Three', 55),\n",
      " ('Thess', 2),\n",
      " ('Thine', 2)]\n",
      "\n",
      "[('their', 2891),\n",
      " ('there', 2280),\n",
      " ('other', 1468),\n",
      " ('those', 1133),\n",
      " ('these', 1002),\n",
      " ('three', 807),\n",
      " ('threw', 102)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# === unit/regression tests\n",
    "print(\"ProbabilityForWord(\\\"the\\\"):\", ProbabilityForWord('the'))\n",
    "print(\"ProbabilityForWord(\\\"pineapple\\\"):\", ProbabilityForWord(\"pineapple\"))\n",
    "print(\"pineapple occurs\", WORD_IDX[\"pineapple\"], \"times in all datasets\")\n",
    "print(\"all words:\", ALL_WORD_CNT)\n",
    "\n",
    "# assertions\n",
    "# (python3 will throw an AssertionError if any \n",
    "#  of these assertions fail)\n",
    "\n",
    "assert(candidates_1(\"pineapple\") == candidates_n(\"pineapple\", 1))\n",
    "\n",
    "# A = 65, # Z = 90\n",
    "for upper_case_letter in range(65, 91):\n",
    "    assert( ProbabilityForWord(upper_case_letter) < 1 )\n",
    "    assert( ProbabilityForWord(upper_case_letter) >= 0 )\n",
    "    assert( WORD_IDX[upper_case_letter] < ALL_WORD_CNT )\n",
    "\n",
    "# a = 97, # z = 122\n",
    "for lower_case_letter in range(97, 123):\n",
    "    assert( ProbabilityForWord(lower_case_letter) < 1 )\n",
    "    assert( ProbabilityForWord(lower_case_letter) >= 0 )\n",
    "    assert( WORD_IDX[lower_case_letter] < ALL_WORD_CNT )\n",
    "\n",
    "common_words = [elmt[0] for elmt in WORD_IDX.most_common(100)]\n",
    "for c in common_words:\n",
    "    assert( ProbabilityForWord(c) < 1)\n",
    "\n",
    "# generate all possible edits for a word\n",
    "# sort these edits by probability\n",
    "tst_word = \"pineappel\"\n",
    "all_edits = [(a, WORD_IDX[a]) for a in only_words_in_dict(candidates_1(tst_word))]\n",
    "all_edits = sorted(all_edits, key=lambda x: x[1], reverse=True)\n",
    "pprint(all_edits)\n",
    "\n",
    "# best seven guesses\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"pineapple\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"Pineappel\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"Pineappee\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"that\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"peel\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"The\")[:7]] )\n",
    "print()\n",
    "pprint( [(x, WORD_IDX[x]) for x in candidates(\"the\")[:7]] )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
