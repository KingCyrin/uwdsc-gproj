{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####### section 00\n",
    "####### loading libraries\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> importing  dataset #1\n",
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
      "\n",
      ">>> importing  dataset #2\n",
      "The frequency distribution for attribute 'lemma' in corpus 'inte\n",
      "\n",
      ">>> importing  dataset #3\n",
      "you 6281002\n",
      "i 5685306\n",
      "the 4768490\n",
      "to 3453407\n",
      "a 3048287\n",
      "it 287996\n",
      "\n",
      ">>> importing  dataset #4\n",
      "2\n",
      "1080\n",
      "&c\n",
      "10-point\n",
      "10th\n",
      "11-point\n",
      "12-point\n",
      "16-point\n",
      "18-point\n",
      "1st\n",
      "\n",
      "\n",
      ">>> importing  dataset #5\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aah\n",
      "aahed\n",
      "aahing\n",
      "aahs\n",
      "aal\n",
      "aalii\n",
      "aaliis\n",
      "aals\n",
      "aam\n",
      "aani\n",
      "aa\n",
      "\n",
      ">>> importing  dataset #6\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aachen\n",
      "aardvark\n",
      "aardvarks\n",
      "aardwolf\n",
      "aardwolves\n",
      "aarhus\n",
      "aa\n",
      "\n",
      ">>> importing  dataset #7\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aachen\n",
      "aardvark\n",
      "aardvarks\n",
      "aaron\n",
      "aback\n",
      "abacus\n",
      "abacuses\n",
      "a\n",
      "\n",
      ">>> importing  dataset #8\n",
      "a\n",
      "aa\n",
      "aaa\n",
      "aachen\n",
      "aardvark\n",
      "aardvarks\n",
      "aaron\n",
      "aback\n",
      "abacus\n",
      "abacuses\n",
      "a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### section 01\n",
    "####### loading databases\n",
    "\n",
    "dataset_lst = []\n",
    "dataset_lst.append(\"yuge_data.txt\")\n",
    "dataset_lst.append(\"internet-en.num.txt\")\n",
    "dataset_lst.append(\"en.txt\")\n",
    "dataset_lst.append(\"words.txt\")\n",
    "dataset_lst.append(\"words_alpha.txt\")\n",
    "dataset_lst.append(\"english3.txt\")\n",
    "dataset_lst.append(\"english2.txt\")\n",
    "dataset_lst.append(\"usa.txt\")\n",
    "\n",
    "# these dont work (because they contain non utf-8 chars)\n",
    "# dataset_lst.append(\"ukenglish.txt\")\n",
    "# dataset_lst.append(\"engmix.txt\")\n",
    "# dataset_lst.append(\"usa2.txt\")\n",
    "\n",
    "yuge_data = \"\"\n",
    "cnt = 1\n",
    "for d in dataset_lst:\n",
    "    data_import = open(\"../datasets/\"+d).read()\n",
    "    yuge_data += data_import\n",
    "    print(\">>> importing  dataset #\" + str(cnt))\n",
    "    print(data_import[:64])\n",
    "    print()\n",
    "\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] the 1st 21 words in all the databases\n",
      "['The',\n",
      " 'Project',\n",
      " 'Gutenberg',\n",
      " 'EBook',\n",
      " 'of',\n",
      " 'The',\n",
      " 'Adventures',\n",
      " 'of',\n",
      " 'Sherlock',\n",
      " 'Holmes',\n",
      " 'by',\n",
      " 'Sir',\n",
      " 'Arthur',\n",
      " 'Conan',\n",
      " 'Doyle',\n",
      " 'in',\n",
      " 'our',\n",
      " 'series',\n",
      " 'by',\n",
      " 'Sir',\n",
      " 'Arthur']\n",
      "[i] retrieve the most common words\n",
      "[('the', 72390),\n",
      " ('of', 39844),\n",
      " ('and', 37067),\n",
      " ('to', 28370),\n",
      " ('in', 20227),\n",
      " ('that', 11994),\n",
      " ('was', 11372),\n",
      " ('he', 10062),\n",
      " ('is', 9648),\n",
      " ('his', 9628),\n",
      " ('with', 9501),\n",
      " ('it', 8572),\n",
      " ('as', 7487),\n",
      " ('had', 7337),\n",
      " ('The', 7245),\n",
      " ('for', 6671),\n",
      " ('by', 6651),\n",
      " ('not', 6485),\n",
      " ('at', 6360)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### section 02\n",
    "####### processing databases\n",
    "\n",
    "def extract_words(src):\n",
    "    res = src\n",
    "\n",
    "    # distinguish between upper/lower case words\n",
    "    res = re.findall(r'\\b\\w+\\b', src)\n",
    "\n",
    "    # filter out numbers\n",
    "    res = list(itertools.filterfalse(lambda x: x.isnumeric(), res))\n",
    "\n",
    "    # filter out words of length 1\n",
    "    res = list(itertools.filterfalse(lambda x: len(x) == 1, res))\n",
    "\n",
    "    return res\n",
    "\n",
    "print(\"[i] the 1st 21 words in all the databases\")\n",
    "pprint( extract_words(yuge_data)[:21] )\n",
    "WORD_IDX = Counter(extract_words(yuge_data))\n",
    "\n",
    "print(\"[i] retrieve the most common words\")\n",
    "pprint( WORD_IDX.most_common(19) )\n",
    "\n",
    "# get the sum of all words\n",
    "ALL_WORD_CNT = sum(WORD_IDX.values())\n",
    "\n",
    "# get the number of occurrences \n",
    "WORD_IDX.values()\n",
    "\n",
    "# probability function\n",
    "# used to help answer the question: how likely is a given word?\n",
    "def ProbabilityForWord(word):\n",
    "    return WORD_IDX[word]/ALL_WORD_CNT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "####### section 03\n",
    "####### building a candidate model\n",
    "        # (this boils down to generating possible \n",
    "        #  words with simple string manipulation)\n",
    "\n",
    "\n",
    "# flatten a list of two tuples\n",
    "def lst_flatten(lst):\n",
    "    ret = []\n",
    "    for a,b in lst:\n",
    "        ret.append(a)\n",
    "        ret.append(b)\n",
    "    return ret\n",
    "\n",
    "def candidates_1(word):\n",
    "    # \"\".join([chr(c) for c in range(65, 91)])\n",
    "    alphab = 'abcdefghijklmnopqrstuvwxyz'+'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "    # edits formed by inserting a space\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    # edits formed by deleting a character\n",
    "    d = [L + R[1:] for L, R in s if R]\n",
    "\n",
    "    # edits formed by swapping the i'th and (i+1)'th characters\n",
    "    # where i < len(word) - 1\n",
    "    t = [L + R[1] + R[0] + R[2:] for L, R in s if len(R)>1] \n",
    "    # ?? consider adding transposes of a longer length \n",
    "    # (i.e., three or four letter transposes)\n",
    "\n",
    "    # edits formed by substituting a random character\n",
    "    # (similar to: \"edits formed by inserting a space\" as shown above)\n",
    "    r = []\n",
    "    for L, R in s:\n",
    "        if R:\n",
    "            for c in alphab:\n",
    "                r.append( L + c + R[1:] )\n",
    "\n",
    "    # edits formed by inserting a random char\n",
    "    i = [L + c + R for L, R in s for c in alphab]\n",
    "\n",
    "    # dont exclude split words\n",
    "    # consider them as candidates\n",
    "    s = lst_flatten(s)\n",
    "\n",
    "    # return all: splits, deletes, edits, replacements, and insertions\n",
    "    return set(s + d + t + r + i)\n",
    "\n",
    "# This code kinda works, but note that when n >= 3, the function \"blows up\".\n",
    "# Finding all edits of distance >= 3 is computationally hard.\n",
    "def candidates_n(word, n):\n",
    "    if (n == 1):\n",
    "        return candidates_1(word)\n",
    "    else: \n",
    "        res = set()\n",
    "        for a in candidates_n(word, n-1):\n",
    "            \n",
    "            # Note: the for loop below is not part of the\n",
    "            # recursion. it's used for flattening the set.\n",
    "            for z in candidates_1(a):\n",
    "                res.add(z)\n",
    "        return res\n",
    "\n",
    "# can a given word be found in the \"dictionary\"?\n",
    "def only_words_in_dict(words):\n",
    "    assert( type(words) == set )\n",
    "    return set(w for w in words if WORD_IDX[w] > 0)\n",
    "\n",
    "def candidates(word):\n",
    "    res = set()\n",
    "\n",
    "    for e1 in only_words_in_dict(candidates_n(word, 1)):\n",
    "        res.add(e1)\n",
    "    for e2 in only_words_in_dict(candidates_n(word, 2)):\n",
    "        res.add(e2)\n",
    "\n",
    "    # res.add(word)\n",
    "\n",
    "    # sort according to the following:\n",
    "        # favour longer words over shorter ones\n",
    "        # favour more frequent words over less frequent words\n",
    "    res = sorted(res, key=lambda elmt:(len(elmt), WORD_IDX[elmt]), reverse=True)\n",
    "\n",
    "    return res\n",
    "\n",
    "def best_guess(word):\n",
    "    candidates(word)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] ProbabilityForWord(\"the\"): 0.026366561125148104\n",
      "[i] ProbabilityForWord(\"pineapple\"): 3.2780639608555458e-06\n",
      "[i] pineapple occurs 9 times in all datasets\n",
      "[i] all words: 2745523\n",
      "\n",
      "[('toothpicks', 6),\n",
      " ('toothstick', 2),\n",
      " ('toothpick', 7),\n",
      " ('tothick', 1),\n",
      " ('toothy', 7),\n",
      " ('tooths', 4),\n",
      " ('tchick', 3)]\n",
      "\n",
      "[('princetown', 3),\n",
      " ('princetons', 1),\n",
      " ('princeton', 6),\n",
      " ('Princeton', 5),\n",
      " ('princedom', 4),\n",
      " ('princekin', 3),\n",
      " ('princes', 15)]\n",
      "\n",
      "[('jupiter', 7),\n",
      " ('jupitar', 1),\n",
      " ('jupatis', 1),\n",
      " ('juplter', 1),\n",
      " ('jupati', 3),\n",
      " ('upstir', 2),\n",
      " ('jumpy', 6)]\n",
      "\n",
      "[('smashup', 3),\n",
      " ('mashlum', 2),\n",
      " ('mashful', 1),\n",
      " ('machuto', 1),\n",
      " ('mashkov', 1),\n",
      " ('masculo', 1),\n",
      " ('maschio', 1)]\n",
      "\n",
      "[('pineapples', 8),\n",
      " ('pineappple', 1),\n",
      " ('pineappies', 1),\n",
      " ('pineapple', 9),\n",
      " ('pineaple', 1),\n",
      " ('pinesap', 2),\n",
      " ('neapple', 1)]\n",
      "\n",
      "[('throat', 81),\n",
      " ('threat', 18),\n",
      " ('thwart', 10),\n",
      " ('thatch', 9),\n",
      " ('thanet', 3),\n",
      " ('thatll', 2),\n",
      " ('thsant', 2)]\n",
      "\n",
      "[('peelers', 6),\n",
      " ('peebles', 3),\n",
      " ('speeled', 3),\n",
      " ('peelite', 2),\n",
      " ('speedle', 1),\n",
      " ('peeleep', 1),\n",
      " ('peehole', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### section 04\n",
    "####### unit testing\n",
    "\n",
    "print(\"[i] ProbabilityForWord(\\\"the\\\"):\", ProbabilityForWord('the'))\n",
    "print(\"[i] ProbabilityForWord(\\\"pineapple\\\"):\", ProbabilityForWord(\"pineapple\"))\n",
    "print(\"[i] pineapple occurs\", WORD_IDX[\"pineapple\"], \"times in all datasets\")\n",
    "print(\"[i] all words:\", ALL_WORD_CNT)\n",
    "\n",
    "# assertions\n",
    "assert(candidates_1(\"pineapple\") == candidates_n(\"pineapple\", 1))\n",
    "\n",
    "for upper_case_letter in range(65, 91): # A = 65, # Z = 90\n",
    "    assert( ProbabilityForWord(upper_case_letter) < 1 )\n",
    "    assert( ProbabilityForWord(upper_case_letter) >= 0 )\n",
    "    assert( WORD_IDX[upper_case_letter] < ALL_WORD_CNT )\n",
    "\n",
    "for lower_case_letter in range(97, 123): # a = 97, # z = 122\n",
    "    assert( ProbabilityForWord(lower_case_letter) < 1 )\n",
    "    assert( ProbabilityForWord(lower_case_letter) >= 0 )\n",
    "    assert( WORD_IDX[lower_case_letter] < ALL_WORD_CNT )\n",
    "\n",
    "# ensure that the most common words \n",
    "# have reasonable probabilities\n",
    "common_words = [elmt[0] for elmt in WORD_IDX.most_common(100)]\n",
    "for c in common_words:\n",
    "    assert( ProbabilityForWord(c) < 1)\n",
    "\n",
    "# for each of these words, generate the best seven guesses\n",
    "print()\n",
    "pprint( [(x, WORD_IDX[x]) for x in candidates(\"toothhick\")[:7]] )\n",
    "print()\n",
    "pprint( [(x, WORD_IDX[x]) for x in candidates(\"princeron\")[:7]] )\n",
    "print()\n",
    "pprint( [(x, WORD_IDX[x]) for x in candidates(\"jupytir\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"mashuo\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"pineappee\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"that\")[:7]] )\n",
    "print()\n",
    "pprint(  [(x, WORD_IDX[x]) for x in candidates(\"peele\")[:7]] )\n",
    "print()\n",
    "\n",
    "# credits to: https://norvig.com/spell-correct.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
