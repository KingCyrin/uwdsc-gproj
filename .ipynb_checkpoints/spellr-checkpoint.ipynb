{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === norvig's code\n",
    "# steal from it without shame xd\n",
    "\n",
    "# import re\n",
    "# from collections import Counter\n",
    "\n",
    "# def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "# def P(word, N=sum(WORDS.values())): \n",
    "#     \"Probability of `word`.\"\n",
    "#     return WORDS[word] / N\n",
    "\n",
    "# def correction(word): \n",
    "#     \"Most probable spelling correction for word.\"\n",
    "#     return max(candidates(word), key=P)\n",
    "\n",
    "# def candidates(word): \n",
    "#     \"Generate possible spelling corrections for word.\"\n",
    "#     return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "# def known(words): \n",
    "#     \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "#     return set(w for w in words if w in WORDS)\n",
    "\n",
    "# def edits1(word):\n",
    "#     \"All edits that are one edit away from `word`.\"\n",
    "#     letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "#     splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "#     deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "#     transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "#     replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "#     inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "#     return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# def edits2(word): \n",
    "#     \"All edits that are two edits away from `word`.\"\n",
    "#     return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten a list of two tuples\n",
    "def lst_flatten(lst):\n",
    "    ret = []\n",
    "    for a,b in lst:\n",
    "        ret.append(a)\n",
    "        ret.append(b)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yuge_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6ac76784b1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"yuge_data.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"internet-en.num.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_import\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0myuge_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> importing  dataset #\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yuge_data.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "yuge_data = \"\"\n",
    "\n",
    "cnt = 1\n",
    "dataset_lst = []\n",
    "dataset_lst.append(\"yuge_data.txt\")\n",
    "dataset_lst.append(\"internet-en.num.txt\")\n",
    "dataset_lst.append(\"en.txt\")\n",
    "dataset_lst.append(\"words.txt\")\n",
    "dataset_lst.append(\"words_alpha.txt\")\n",
    "\n",
    "for d in dataset_lst:\n",
    "    data_import = open(d).read()\n",
    "    yuge_data += data_import\n",
    "    print(\">>> importing  dataset #\" + str(cnt))\n",
    "    print(data_import[:128])\n",
    "    print()\n",
    "\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_words(src):\n",
    "    # allow upper case words\n",
    "    return re.findall(r'\\b\\w+\\b', src)\n",
    "#     return re.findall(r'\\b\\w+\\b', src.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a regex test\n",
    "# tst_str = \"The Project Gutenberg EBook of The Adventures of Sherlock Holmes\\nby Sir Arthur Conan Doyle\\n(#15 in our series by Sir Arthur Conan Doyle)\\n\\nCopyright laws are changing all over the world. Be sure to check the\\ncopyright laws for your country before downloadin\"\n",
    "# re.findall(r'\\b\\w+\\b',tst_str)\n",
    "\n",
    "extract_words(yuge_data)\n",
    "WORD_IDX = Counter(extract_words(yuge_data))\n",
    "\n",
    "# retrieve the 7 most common words\n",
    "WORD_IDX.most_common(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how likely is a given word?\n",
    "\n",
    "# get the sum of all words\n",
    "ALL_WORD_CNT = sum(WORD_IDX.values())\n",
    "\n",
    "# get the number of occurrences \n",
    "WORD_IDX.values()\n",
    "\n",
    "# probability function\n",
    "def ProbabilityForWord(word):\n",
    "    return WORD_IDX[word]/ALL_WORD_CNT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# building our candidate model\n",
    "# (this boils down to generating possible words \n",
    "#  based on simple string manipulation)\n",
    "\n",
    "\n",
    "# [print(x) for x in s]\n",
    "# print()\n",
    "# [print(x) for x in d]\n",
    "# print()\n",
    "# [print(x) for x in t]\n",
    "# print()\n",
    "# [print(x) for x in r]\n",
    "# print()\n",
    "# [print(x) for x in i]\n",
    "# print()\n",
    "\n",
    "def candidates_1(word):\n",
    "    # \"\".join([chr(c) for c in range(65, 91)])\n",
    "    alphab = 'abcdefghijklmnopqrstuvwxyz'+'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "    # edits formed by inserting a space\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    # edits formed by deleting a character\n",
    "    d = [L + R[1:] for L, R in s if R]\n",
    "\n",
    "    # edits formed by swapping the i'th and (i+1)'th characters\n",
    "    # where i < len(word) - 1\n",
    "    t = [L + R[1] + R[0] + R[2:] for L, R in s if len(R)>1] \n",
    "    # TODO: consider adding transposes of a longer length \n",
    "    # (i.e., three or four letter transposes)\n",
    "\n",
    "    # edits formed by substituting a random character\n",
    "    # (similar to: \"edits formed by inserting a space\" as shown above)\n",
    "    r = []\n",
    "    for L, R in s:\n",
    "        if R:\n",
    "            for c in alphab:\n",
    "                r.append( L + c + R[1:] )\n",
    "\n",
    "    # edits formed by inserting a random char\n",
    "    i = [L + c + R for L, R in s for c in alphab]\n",
    "\n",
    "    # dont exclude split words\n",
    "    # consider them as candidates\n",
    "    s = lst_flatten(s)\n",
    "\n",
    "    # return all: splits, deletes, edits, replacements, and insertions\n",
    "    return set(s + d + t + r + i)\n",
    "\n",
    "'''\n",
    "# TODO: define a recursive candidate function\n",
    "# (this kinda like DFS)\n",
    "def candidates_n(word, n):\n",
    "    if (n == 1):\n",
    "        candidates_1(word)\n",
    "    else:\n",
    "        for a in candidates_1(word):\n",
    "            for b in candidates\n",
    "\n",
    "# tests\n",
    "# candidates_1(\"pineapple\")\n",
    "# candidates_n(\"pineapple\", 1)\n",
    "# candidates_n(\"q\", 1)\n",
    "'''\n",
    "\n",
    "# TODO: fix this (not working for some reason)\n",
    "# def candidates_2(word):\n",
    "#     \"All edits that are two edits away from `word`.\"\n",
    "#     return (e2 for e1 in candidates_1(word) for e2 in candidates_1(e1))\n",
    "\n",
    "# candidates_2(\"pineapple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can a given word be found in the \"dictionary\"?\n",
    "def only_words_in_dict(words):\n",
    "    assert( type(words) == set )\n",
    "    return set(w for w in words if WORD_IDX[w] > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### === unit/regression tests\n",
    "\n",
    "print(\"prob(the)\", ProbabilityForWord('the'))\n",
    "print(\"prob(pineapple)\", ProbabilityForWord(\"pineapple\"))\n",
    "print(\"pineapple\", WORD_IDX[\"pineapple\"])\n",
    "print(\"all words\", ALL_WORD_CNT)\n",
    "\n",
    "# A = 65\n",
    "# Z = 90\n",
    "for upper_case_letter in range(65, 91):\n",
    "    assert( ProbabilityForWord(upper_case_letter) < 1 )\n",
    "    assert( ProbabilityForWord(upper_case_letter) >= 0 )\n",
    "    assert( WORD_IDX[upper_case_letter] < ALL_WORD_CNT )\n",
    "\n",
    "# a = 97\n",
    "# z = 122\n",
    "for lower_case_letter in range(97, 123):\n",
    "    assert( ProbabilityForWord(lower_case_letter) < 1 )\n",
    "    assert( ProbabilityForWord(lower_case_letter) >= 0 )\n",
    "    assert( WORD_IDX[lower_case_letter] < ALL_WORD_CNT )\n",
    "\n",
    "common_words = [\"the\", \"i\", \"I\", \"am\"]\n",
    "for c in common_words:\n",
    "    assert( ProbabilityForWord(c) < 1)\n",
    "\n",
    "# generate all possible edits for a word\n",
    "tst_word = \"pineapple\"\n",
    "[(a, WORD_IDX[a]) for a in only_words_in_dict(candidates_1(tst_word))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
